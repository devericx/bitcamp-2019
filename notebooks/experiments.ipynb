{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 11.802986145019531 secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sa carte huit cœur\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "import support.scrape as scrape\n",
    "\n",
    "from pywsd.similarity import max_similarity,similarity_by_path\n",
    "from pywsd.lesk import simple_lesk, adapted_lesk, cosine_lesk,original_lesk\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer,sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import string\n",
    "import operator\n",
    "\n",
    "\n",
    "from frenetic.frenetic import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Languages\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "\n",
    "#targetLang = \"english\"\n",
    "targetLang = \"french\"\n",
    "\n",
    "\n",
    "fwn = None\n",
    "if targetLang == \"french\":\n",
    "    fwn = FreNetic(\"./frenetic/data/wolf.xml\") \n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(targetLang)) \n",
    "\n",
    "def simpleFilter(sentence):\n",
    "    filtered_sent = []\n",
    "    \n",
    "    lemmatizer = None\n",
    "    if targetLang == \"french\":\n",
    "        lemmatizer = FrenchLefffLemmatizer()\n",
    "    else:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    \n",
    "    stop_words = set(stopwords.words(targetLang))\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "    return filtered_sent\n",
    "\n",
    "def listToString(inputSentList):\n",
    "    return ' '.join(inputSentList)\n",
    "\n",
    "def viewAllDefinitions(wordStr):\n",
    "    resList = []\n",
    "    for syn in wn.synsets(wordStr):\n",
    "        resList.append(syn.definition())\n",
    "    return resList\n",
    "\n",
    "def topRelevSent(targetWord):\n",
    "    return\n",
    "\n",
    "def simpleFilter(sentence):\n",
    "    filtered_sent = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(targetLang))\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(lemmatizer.lemmatize(w))\n",
    "    return listToString(filtered_sent)\n",
    "\n",
    "def simpleFilterList(sentList):\n",
    "    resList = []\n",
    "    for sent in sentList:\n",
    "        sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
    "        resList.append(simpleFilter(sent))\n",
    "    return resList\n",
    "\n",
    "def combineWordVec(a,b,aVec,bVec):\n",
    "    aDf = pd.DataFrame(a.toarray(),columns=aVec.get_feature_names())\n",
    "    bDf = pd.DataFrame(b.toarray(),columns=bVec.get_feature_names())\n",
    "\n",
    "    combinedDf = pd.concat([aDf, bDf],sort=False).fillna(value=0.0)\n",
    "    aRes = combinedDf.iloc[0].values\n",
    "    bRes = combinedDf.iloc[1].values\n",
    "    return aRes,bRes\n",
    "    \n",
    "def calcCosSim(a,b):\n",
    "    # Calculate cosine simularity\n",
    "    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    \n",
    "def leskAlgoFr(sentence,ambiguous):\n",
    "    synsets = fwn.synsets(\"cœur\")\n",
    "    \n",
    "    sentence = simpleFilter(sentence)\n",
    "    \n",
    "    resAr = []\n",
    "    for test in synsets:\n",
    "        # We calculate cosine simularity\n",
    "        vecSent = CountVectorizer()\n",
    "        sentenceFreq = vecSent.fit_transform([sentence])\n",
    "        \n",
    "        definition = test.defn()\n",
    "        \n",
    "        vecDef = CountVectorizer()\n",
    "        definitionFreq = vecDef.fit_transform([definition])\n",
    "        \n",
    "        defRes,sentRes = combineWordVec(definitionFreq,sentenceFreq,vecDef,vecSent)\n",
    "        \n",
    "        res = calcCosSim(defRes,sentRes)\n",
    "        resAr.append(res)\n",
    "        \n",
    "    return synsets[np.argmax(np.asarray(resAr))].defn()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "sent = 'His card was the 8 of hearts'\n",
    "ambiguous = 'heart'\n",
    "'''\n",
    "\n",
    "\n",
    "sent = 'Sa carte était la huit de cœur'\n",
    "ambiguous = 'cœur'\n",
    "\n",
    "\n",
    "sent = simpleFilter(sent)\n",
    "\n",
    "\n",
    "#cosine_lesk(sent,ambiguous).definition()\n",
    "print(sent)\n",
    "print(cosine_lesk(sent,ambiguous))\n",
    "\n",
    "\n",
    "# original_lesk seems to be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'une carte à jouer dans le costume majeur qui a un ou plusieurs coeurs rouges dessus'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "leskAlgoFr(sent,ambiguous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng-30-03107904-n\n",
      "la chambre d'un réacteur nucléaire contenant la matière fissile où la réaction a lieu\n",
      "eng-30-03507048-n\n",
      "une carte à jouer dans le costume majeur qui a un ou plusieurs coeurs rouges dessus\n",
      "eng-30-04624826-n\n",
      "une inclination ou tendance d'un certain genre\n",
      "eng-30-04857490-n\n",
      "le courage de continuer\n",
      "eng-30-05388805-n\n",
      "l'organe musculaire creux situé derrière le sternum et entre les poumons; ses contractions rythmiques déplacent le sang à travers le corps\n",
      "eng-30-05919263-n\n",
      "the locus of feelings and intuitions\n",
      "eng-30-05921123-n\n",
      "the choicest or most essential or most vital part of some idea or experience\n",
      "eng-30-07544647-n\n",
      "a positive feeling of liking\n",
      "eng-30-07651905-n\n",
      "a firm rather dry variety meat (usually beef or veal)\n",
      "eng-30-13865904-n\n",
      "a plane figure with rounded sides curving inward at the top and intersecting at the bottom; conventionally used on playing cards and valentines\n"
     ]
    }
   ],
   "source": [
    "synsets = fwn.synsets(\"cœur\")\n",
    "\n",
    "for test in synsets:\n",
    "    print(test.sid())\n",
    "    print(test.defn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sloping land (especially the slope beside a body of water)',\n",
       " 'a financial institution that accepts deposits and channels the money into lending activities',\n",
       " 'a long ridge or pile',\n",
       " 'an arrangement of similar objects in a row or in tiers',\n",
       " 'a supply or stock held in reserve for future use (especially in emergencies)',\n",
       " 'the funds held by a gambling house or the dealer in some gambling games',\n",
       " 'a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force',\n",
       " 'a container (usually with a slot in the top) for keeping money at home',\n",
       " 'a building in which the business of banking transacted',\n",
       " 'a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)',\n",
       " 'tip laterally',\n",
       " 'enclose with a bank',\n",
       " 'do business with a bank or keep an account at a bank',\n",
       " 'act as the banker in a game or in gambling',\n",
       " 'be in the banking business',\n",
       " 'put into a bank account',\n",
       " 'cover with ashes so to control the rate of burning',\n",
       " 'have confidence or faith in']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "        \n",
    "viewAllDefinitions('bank')     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk\n",
    "sent = 'I went to the bank to deposit my money'\n",
    "ambiguous = 'bank'\n",
    "original_lesk(sent,ambiguous).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_lesk('my cat likes to eat mice', 'cat').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent = 'She is mean to me'\n",
    "#sent = 'She mean me'\n",
    "sent = 'What do you mean'\n",
    "\n",
    "original_lesk(sent, 'mean').definition()\n",
    "#simple_lesk(sent, 'mean').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'run'  'advice' , 'bat', 'project'  'fine'  'consoil'  'roll'  ''\n",
    "sent = 'The rebel seized the opportunity to rebel'\n",
    "adapted_lesk(sent, 'rebel').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperateByDef(targetWord):\n",
    "    # Returns a dictionary sorted by defintion\n",
    "    \n",
    "    sentList = None\n",
    "    if targetLang == \"french\":\n",
    "        \n",
    "        sentList = scrape.scrapeFR(targetWord)\n",
    "    else:\n",
    "        sentList = scrape.scrape(targetWord)\n",
    "    \n",
    "    dictDef = {}\n",
    "    for i,sent in enumerate(sentList):\n",
    "        defineSent = None\n",
    "        if targetLang == \"french\":\n",
    "            defineSent = leskAlgoFr(sent,ambiguous) \n",
    "        else:\n",
    "            #defineSent = cosine_lesk(sent,targetWord).definition()\n",
    "            defineSent = adapted_lesk(sent,targetWord).definition()        \n",
    "        \n",
    "        if defineSent not in dictDef:\n",
    "            dictDef[defineSent] = [sent]\n",
    "        else:\n",
    "            dictDef[defineSent].append(sent)\n",
    "    return dictDef\n",
    "\n",
    "def printDictKeysVals(inputDict):\n",
    "    for key in inputDict.keys():\n",
    "        print('-------------------------------------------------------')\n",
    "        print('Def: ', key)\n",
    "        for value in inputDict[key]:\n",
    "            print('        ', value)\n",
    "    \n",
    "def getDef(sent,targetWord):\n",
    "    # Get defintion of word\n",
    "    \n",
    "    if targetLang == \"french\":\n",
    "        defineSent = leskAlgoFr(sent,ambiguous) \n",
    "    else:\n",
    "        #defineSent = cosine_lesk(sent,targetWord).definition()\n",
    "        #defineSent = adapted_lesk(sent,targetWord).definition() \n",
    "        defineSent = original_lesk(sent,targetWord).definition() \n",
    "    \n",
    "    return defineSent\n",
    "\n",
    "def printDefExampleSent(inputDict,wordDef):\n",
    "    print('Definition: ')\n",
    "    print(wordDef)\n",
    "    print('\\n\\n\\n')\n",
    "    print('Example sentence:')\n",
    "    \n",
    "    \n",
    "    \n",
    "    if wordDef not in inputDict:\n",
    "        print('No example sentences found')\n",
    "        return\n",
    "    \n",
    "    for sent in inputDict[wordDef]:\n",
    "        print('-',sent)\n",
    "        print('')\n",
    "        \n",
    "        \n",
    "def countDefFreq(inputDict,n):\n",
    "    resDict = {}\n",
    "    for key in inputDict.keys():\n",
    "        resDict[key] = len(inputDict[key])\n",
    "    return getTopDefEntries(resDict, n)\n",
    "\n",
    "def getTopDefEntries(inputDict, n):\n",
    "    return dict(sorted(inputDict.items(), key=operator.itemgetter(1), reverse=True)[:n])\n",
    "\n",
    " \n",
    "targetWord = 'cœur'\n",
    "#testSent = 'He does not have money in his bank account'\n",
    "testSent = 'Sa carte était la huit de cœur'\n",
    "\n",
    "\n",
    "\n",
    "dictDef = seperateByDef(targetWord)\n",
    "wordDef = getDef(testSent,targetWord)\n",
    "\n",
    "#printDefExampleSent(dictDef,wordDef)\n",
    "#printDictKeysVals(dictDef)\n",
    "\n",
    "\n",
    "\n",
    "# Print out frequency of each definiton used\n",
    "topRes = countDefFreq(dictDef, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l'organe musculaire creux situé derrière le sternum et entre les poumons; ses contractions rythmiques déplacent le sang à travers le corps :  12\n",
      "la chambre d'un réacteur nucléaire contenant la matière fissile où la réaction a lieu :  11\n",
      "une inclination ou tendance d'un certain genre :  2\n",
      "le courage de continuer :  1\n",
      "une carte à jouer dans le costume majeur qui a un ou plusieurs coeurs rouges dessus :  1\n"
     ]
    }
   ],
   "source": [
    "for key in topRes.keys():\n",
    "    print(key,': ', topRes[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Les chemins de fer ont été au cœur de la révolution industrielle.',\n",
       " 'En ce qui concerne, notamment, la qualité du matériau à utiliser, les deux qualités (cœur blanc et cœur noir) sont autorisées.',\n",
       " \"Chacune d'entre elles doit se concentrer sur ses qualités et son savoir-faire propres - sur son cœur de métier.\",\n",
       " \"Le cœur de l'enseignement de Jésus, avec ses demandes exigeantes en ce qui regarde le Royaume de Dieu, est un appel à la metanoia, même si le terme « conversion » n'est pas toujours employé.\",\n",
       " \"Le développement de l'Espace Européen de la Recherche et de l'Innovation nous tient à cœur afin de rendre l'Europe plus compétitive, une Europe basée sur la connaissance et l'innovation.\",\n",
       " \"Le corps matériel se recroquevilla suite à ce retour précipité de l'esprit, si bien qu'il se mit à suer et que son cœur commença à battre la chamade.\",\n",
       " \"Il est temps de prendre les choses à cœur et de montrer la volonté politique nécessaire qui nous permettra de consolider la base de l'élimination de toutes les armes nucléaires et de destruction massive.\",\n",
       " \"En dépit de toutes les vicissitudes de la vie, que votre cœur s'épanouisse dans l'amour et la compassion.\",\n",
       " \"En outre, les employés et les retraités ont vraiment prouvé qu'ils avaient le cœur sur la main en dépassant l'objectif fixé de plus de 67 000 $.\",\n",
       " \"Le Conseil appelle à nouveau les dirigeants nouvellement élus du pays à engager un dialogue politique constructif afin de développer une vision commune pour l'avenir du pays et d'ancrer le programme de l'UE au cœur du programme de ses gouvernements.\",\n",
       " \"Un changement qui place les besoins des femmes au cœur de la sécurité dans les situations de conflit et d'après-conflit.\",\n",
       " \"Selon lui, les arts sont au cœur de l'enseignement et toutes les autres matières devraient être enseignées à travers eux.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictDef[list(topRes.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize)\n",
    "\n",
    "def get_clusters(sentences):\n",
    "    tf_idf_matrix = vectorizer.fit_transform(sentences)\n",
    "    similarity_matrix = (tf_idf_matrix * tf_idf_matrix.T).A\n",
    "    affinity_propagation = AffinityPropagation(damping=0.5)\n",
    "    affinity_propagation.fit(similarity_matrix)\n",
    "    print(similarity_matrix.shape)\n",
    "\n",
    "    labels = affinity_propagation.labels_\n",
    "    cluster_centers = affinity_propagation.cluster_centers_indices_\n",
    "\n",
    "    tagged_sentences = zip(sentences, labels)\n",
    "    clusters = {}\n",
    "\n",
    "    for sentence, cluster_id in tagged_sentences:\n",
    "        clusters.setdefault(sentences[cluster_centers[cluster_id]], []).append(sentence)\n",
    "\n",
    "    return clusters, affinity_propagation\n",
    "\n",
    "sentences = scrape.scrape('cat')\n",
    "\n",
    "\n",
    "#sentences = simpleFilterList(sentences)\n",
    "\n",
    "clusters, affinity_propagation = get_clusters(sentences)\n",
    "print(len(clusters))\n",
    "for cluster in clusters:\n",
    "    print('--------------------------------------')\n",
    "    print(cluster, ':')\n",
    "    for element in clusters[cluster]:\n",
    "        print('  - ', element)\n",
    "\n",
    "tf_idf_predMat = vectorizer.fit_transform(['The cat is running away from the dog'])\n",
    "similarity_matrixPred = (tf_idf_predMat.T*tf_idf_predMat).A\n",
    "#print(affinity_propagation.predict(similarity_matrixPred))\n",
    "print(similarity_matrixPred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer0 = TfidfVectorizer(tokenizer=normalize)\n",
    "sentences = scrape.scrape('cat')\n",
    "\n",
    "tf_idf_matrix0 = vectorizer0.fit_transform(sentences)\n",
    "similarity_matrixCluster = (tf_idf_matrix0.T * tf_idf_matrix0).A\n",
    "affinity_propagation0 = AffinityPropagation(damping=0.5)\n",
    "affinity_propagation0.fit(similarity_matrixCluster)\n",
    "        \n",
    "\n",
    "    \n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize)\n",
    "tf_idf_matrix = vectorizer.fit_transform(['The cat is running away from the dog'])\n",
    "similarity_matrixPred = (tf_idf_predMat.T*tf_idf_predMat).A\n",
    "#print(affinity_propagation.predict(similarity_matrix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "similarity_matrixPredFlat = np.sum(similarity_matrixPred,axis=0)/similarity_matrixPred.shape[0]\n",
    "\n",
    "\n",
    "predDf = pd.DataFrame(columns=vectorizer.get_feature_names())\n",
    "predDf.loc[len(predDf)] = similarity_matrixPredFlat\n",
    "\n",
    "similarity_matrixCluster = np.sum(similarity_matrixCluster,axis=0)/similarity_matrixCluster.shape[0]\n",
    "\n",
    "clusterDf = pd.DataFrame(columns=vectorizer0.get_feature_names())\n",
    "clusterDf.loc[len(clusterDf)] = similarity_matrixCluster\n",
    "\n",
    "combinedDf = pd.concat([predDf , clusterDf],sort=False).fillna(value=0.0)\n",
    "\n",
    "predVec = combinedDf.iloc[0].values\n",
    "clusterVec = combinedDf.iloc[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similarity_matrixCluster.shape)\n",
    "#print(affinity_propagation.predict(predVec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfPredMat = predVec.tolist()*predVec.transpose()\n",
    "predVec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
